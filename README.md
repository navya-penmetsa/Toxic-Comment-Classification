Toxic Comment Classification
   
Source: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification
   
Description: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/description
   
Problem Statement: This project’s focus is to solve the Toxic comment classification challenge posted on Kaggle. The Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity.

Introduction: In this project, the attempt is identification of toxic online comments in a dataset provided by a Kaggle challenge. The dataset consists of 159,571 comments from Wikipedia talk page edits which have been labelled by human raters for the presence of toxic behavior. The 6 types of toxicity are: toxic, severe_toxic, obscene, threat, insult and identity_hate. Here the challenge is to build a multi-headed model that’s capable of detecting different types of toxicity like threats, obscenity, insults, and identity-based hate. So the objective of the project is to build a multi-label classification of toxic comments. 

Pre-Processing: Tokenization and embedding is done on the dataset. Tokenization is the process of converting a text corpus to a set of distinct tokens of any size. These tokens are usually numbers which are assigned to the words present in the text. As a computer cannot understand a language, this method helps us to map all the words to distinct numbers which makes it easier for the computer to understand. So the result of this process is a dictionary of fixed size that contains a mapping from words to numbers. Every word in the dataset is embedded into feature vectors, this is done by creating an embedding matrix. An embedding matrix is a list of words and their corresponding embeddings. Embeddings usually refer to n-dimensional dense vectors. The embedding matrix is of shape (max_features, embedding_dim). Here max_features is the number of words in the dictionary that are obtained from the tokenization method and embedding_dim is the number of features into which the words will be embedded. This project uses GloVe. GloVe word embedding is downloaded and a dictionary is created with those embeddings. In the next step word embedding matrix for each word is created and if the word doesn’t have an embedding in GloVe it will be presented with a zero matrix. 

Methodology: This project will describe a baseline approach to this multi-label classification task using Convolution Neural Network (CNN) and Bi-directional Gated Recurrent Unit (bi-GRU).

Convolutional neural networks (CNN) has been widely used for Image recognition or Image Processing in computer vision, it is core most model used in computer vision. Recently convolutional neural networks (CNN) have been interestingly used in NLP and have gained quite good and accurate results. Convolutional Neural Networks (CNN) has been widely applied for classification problems in different fields like text classification. 

Models with bi-directional structure have the ability to learn information from previous and subsequent data when dealing with the current data. The bi-GRU model is determined based on the state of two GRUs, which are unidirectional in opposite directions. One GRU that moves forward, beginning from the start of the data sequence, the other GRU that moves backward, beginning from the end of the data sequence. This allows the information from both future and past to impact the current states. The bi-directional Gated Recurrent Unit (bi-GRU) can capture long range dependencies with lesser model parameters.
   
